\documentclass{article}
\usepackage{listings}
\usepackage{natbib}   % For citation management
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{config/psl-cover/aas_macros}
\usepackage{url}
\geometry{a4paper,
  left= 3cm,right = 2cm,  
  top = 3cm,bottom = 3cm,
  headheight=6mm,         
  marginparwidth = 16mm}

\begin{document}
\section{Tstrippy}

    This project is built on \texttt{f2py}, which allows integration between Fortran and Python. The core motivation behind this choice is performance: Fortran, as a compiled language, provides significantly faster execution for numerically intensive tasks, while Python—especially within Jupyter notebooks—offers a convenient environment for development, experimentation, and visualization. \texttt{F2py} stands for \textit{Fortran to Python} \citep{peterson2009f2py}, and it is included as a module within NumPy \citep{numpy_f2py_manual,2020Natur.585..357H}. The name of the project, \texttt{tstrippy}, stands for \texttt{T}idal \texttt{Strip}ping in \texttt{Py}thon.

    \texttt{F2py} supports Fortran 77, 90, and 95 standards, so we chose to write the code in Fortran 90 to make use of its support for \textit{modules}. In Fortran, a module encapsulates data and subroutines in a manner somewhat analogous to classes in object-oriented programming. However, Fortran modules do not support inheritance, and only a single instance of a module can exist at a time—unlike classes, which can be instantiated multiple times.

    \texttt{Tstrippy} package is structured around five core Fortran modules, each responsible for a distinct aspect of the simulation:
    \begin{itemize}
        \item \texttt{integrator}: This is the central module of the code. It stores particle positions and velocities, computes forces, and evolves the system forward in time. It also handles the writing of output data at specified intervals and interfaces with all other modules in the code.
        \item \texttt{potentials}: This module defines the analytical potentials used to compute gravitational forces. It currently supports several models, including \texttt{Plummer}, \texttt{Hernquist}, \texttt{AllenSantillian}, \texttt{MiyamotoNagai}, the bar model \texttt{LongMuraliBar} from \citet{1992ApJ...397...44L}, and the composite model from \texttt{pouliasis2017pii} \citep{2017A&A...598A..66P}. This module can also be called in Python, allowing users to call potential functions directly, e.g., for computing energies during post-processing.
        \item \texttt{hostperturber}: This module handles the host globular cluster. It stores its orbit (i.e., timestamps, positions, and velocities) and ensures its synchronization with the simulation's internal clock. It computes the gravitational influence of the host cluster on each star particle. This is an internal module only.
        \item \texttt{perturbers}: Similar in function to \texttt{hostperturber}, this module supports additional perturbing clusters. It allows for the inclusion of multiple perturbers and computes their collective force on each particle. If object-oriented programming were available in Fortran, both this module and \texttt{hostperturber} would naturally inherit from a shared parent class. This module only uses the positions, masses, and characteristic radii of the other perturbers. The velocities are not imported.
        \item \texttt{galacticbar}: This module stores parameters for the Galactic bar, including the polynomial coefficients for its angular displacement as a function of time:
        \[
        \theta(t) = \theta_0 + \omega t + \dot{\omega} t^2 + \ddot{\omega} t^3 + \dots
        \]
        It performs transformations into the rotating bar frame, computes the forces in that frame, and then transforms the forces back to the Galactocentric reference frame.
    \end{itemize}
    This modular structure makes it straightforward to extend the code by adding new physics in the form of additional modules.

    To make the package installable and easy to distribute, I initially followed the guide by \citet{pythonpackagingguide}, which describes how to create a Python package using \texttt{setuptools}—the standard build system in the Python ecosystem. However, compatibility between \texttt{setuptools} and \texttt{f2py} was broken starting with NumPy~$>$~1.22 (released June 22, 2022\footnote{\url{https://github.com/numpy/numpy/releases/tag/v1.22.0}}) and Python~$>$~3.9.18 (released June 24, 2024\footnote{\url{https://www.python.org/downloads/release/}}). This meant that Fortran extensions could only be compiled using deprecated versions of both. These older versions of NumPy were also not compatible with Apple's ARM-based M1 and M2 processors, rendering the code unusable on modern Mac systems.

    This limitation stemmed from the deprecation and eventual removal of \texttt{numpy.distutils}, the tool that previously enabled seamless integration of Fortran code in NumPy-based packages. As of NumPy~1.23 and later, \texttt{numpy.distutils} was deprecated, and with NumPy~2.0, it was removed entirely. The NumPy developers recommended migrating to \texttt{meson} \citep{meson_manual} or \texttt{Cmake}.

    To address these issues, I migrated the build process to \texttt{meson}, a language-agnostic build system capable of compiling Fortran, C, and Python extensions across architectures. This eliminated compatibility problems and made the build process architecture-independent. The build system now automatically detects the system architecture and compiles accordingly.

    The code is fully open source and available on GitHub.\footnote{\url{https://github.com/salvatore-ferrone/tstrippy}} To support users, I created documentation hosted on \texttt{readthedocs.io}\footnote{\url{https://tstrippy.readthedocs.io/en/latest/}}, which includes working examples and basic usage guides. A minimal test suite, written using \texttt{pytest}, is also included. While not exhaustive, the tests ensure that core functionality remains intact as the code evolves. In the next section, I present a minimal example of how the user may use the code within a python script.

    \subsection{Minimum example}
        If the package is properly installed on the system, it can be imported at the top of any Python script:
        \small
        \begin{lstlisting}[language=python]
            import tstrippy    
        \end{lstlisting}
        \normalsize
        Next, the user must load or define the initial conditions. The code provides:
        \begin{itemize}
            \item The masses, sizes, and kinematics of the globular cluster catalog from \citet{2018MNRAS.478.1520B};
            \item The galactic potential parameters for model II of \citet{2017A&A...598A..66P}; and
            \item A galactic reference frame.
        \end{itemize}

        \small
        \begin{lstlisting}[language=python]
            GCdata      = \
                tstrippy.Parsers.baumgardtMWGCs().data
            MWparams    = \
                tstrippy.Parsers.potential_parameters.pouliasis2017pii()
            MWrefframe  = \
                tstrippy.Parsers.potential_parameters.MWreferenceframe()
        \end{lstlisting}
        \normalsize
        The user must then select the system to integrate. For example, to integrate the orbits of observed globular clusters, one must convert the ICRS coordinates to a Galactocentric frame using \texttt{astropy} and the provided MW reference frame. Alternatively, to simulate a star cluster, one can generate a Plummer sphere:
        \small
        \begin{lstlisting}[language=python]
            xp,yp,zp,vxp,vyp,vzp=\
            tstrippy.ergodic.isotropicplummer(G,massHost,halfmassradius,NP)
        \end{lstlisting}
        Here, \texttt{NP} is the number of particles, \texttt{halfmassradius} is the system's half-mass radius, \texttt{massHost} is the total mass of the Plummer sphere, and \texttt{G} is the gravitational constant. All values must be in the same unit system.

        The integrator must then be initialized. All parameters are passed via lists that are unpacked at the function call. Here is an example of initializing the integrator for a stellar stream in a potential that includes a rotating galactic bar:
        \small
        \begin{lstlisting}[language=python]
            tstrippy.integrator.setstaticgalaxy(*staticgalaxy)
            tstrippy.integrator.setinitialkinematics(*initialkinematics)
            tstrippy.integrator.setintegrationparameters(*integrationparameters)
            tstrippy.integrator.inithostperturber(*hostperturber)
            tstrippy.integrator.initgalacticbar(*galacticbar)
            tstrippy.integrator.setbackwardorbit()
        \end{lstlisting}
        \normalsize

        \begin{itemize}
            \item \texttt{setstaticgalaxy} specifies the static potential model and passes its parameters.
            \item \texttt{setinitialkinematics} provides the initial positions and velocities of the particles.
            \item \texttt{setintegrationparameters} defines the initial time, timestep, and number of steps.
            \item \texttt{inithostperturber} specifies the globular cluster’s trajectory and mass as a function of time.
            \item \texttt{initgalacticbar} defines a rotating bar. It takes the name of the bar model, potential parameters, and spin parameters.
            \item \texttt{setbackwardorbit} reverses the velocity vectors and sets the internal clock to count down: $t_i = t_0 - i \cdot \Delta t$. For the usecase presented in this work, \texttt{setbackwardorbit} is used for computing the globular cluster orbits and not for the star-particles. 
        \end{itemize}

        The user can choose between two output modes during integration:
        \begin{lstlisting}
            tstrippy.integrator.initwriteparticleorbits(nskip,myoutname,myoutdir)
            tstrippy.integrator.initwritestream(nskip,myoutname,myoutdir)
        \end{lstlisting}
        Conceptually, these represent two output paradigms:
        \begin{itemize}
            \item \texttt{initwriteparticleorbits} saves the full orbit of each particle to an individual file.
            \item \texttt{initwritestream} saves full snapshots of all particles at selected timesteps.
        \end{itemize}

        Both functions take:
        \begin{itemize}
            \item \texttt{nskip}: number of timesteps to skip between outputs;
            \item\texttt{myoutname}: the base file name;
            \item \texttt{myoutdir}: the output directory.
        \end{itemize}

        The output files will be named like: \texttt{../dir/temp0.bin}, \texttt{../dir/temp1.bin}, ..., up to \texttt{../dir/tempN.bin}, where $N = N_\mathrm{step} / N_\mathrm{skip}$. Note that the files are written in Fortran binary format. Although \texttt{scipy.io.FortranFile} can read them, I use a custom parser based on \texttt{numpy.frombuffer} to avoid the SciPy dependency.

        Once all parameters are set, the user can proceed with integration using one of two methods:

        \subsubsection*{Full orbit integration (in memory)}
        \small
        \begin{lstlisting}
            xt,yt,zt,vxt,vyt,vzt=\
                tstrippy.integrator.leapfrogintime(Ntimestep,nObj) 
            timestamps=\
                tstrippy.integrator.timestamps.copy()
        \end{lstlisting}
        \normalsize
        \texttt{leapfrogintime} stores the full orbit of each particle in memory. This is useful for a small number of particles or short integrations—e.g., rapid parameter studies in a notebook. However, for large simulations it can be prohibitively memory-intensive. For instance, integrating all globular clusters at high time resolution might require:
        \begin{equation}
            7 \times N_p \times N_\mathrm{step} \times 8~\mathrm{Byte} \approx 450~\mathrm{GB}
        \end{equation}
        if $N_\mathrm{step} \approx 10^7$. This will likely exceed system RAM.

        \subsubsection*{Final state only}
        \small
        \begin{lstlisting}
            tstrippy.integrator.leapfrogtofinalpositions()
            xf  = tstrippy.integrator.xf.copy()
            yf  = tstrippy.integrator.yf.copy()
            zf  = tstrippy.integrator.zf.copy()
            vxf = tstrippy.integrator.vxf.copy()
            vyf = tstrippy.integrator.vyf.copy()
            vzf = tstrippy.integrator.vzf.copy()
            finaltime=tstrippy.integrator.currenttime.copy()
        \end{lstlisting}
        \normalsize
        \texttt{leapfrogtofinalpositions()} performs the integration but only returns the final phase-space coordinates. These arrays must be copied before deallocating memory:
        \small
        \begin{lstlisting}
            tstrippy.integrator.deallocate()
        \end{lstlisting}
        \normalsize
        Deallocating is necessary to avoid memory leaks or crashes in Jupyter when rerunning code cells.

    \subsection{Reflection on developping \texttt{tstrippy}}

        The earliest version of this code began as a simple Fortran script, built to integrate test-particles in our specific gravitational potential models. Since I was already familiar with Fortran and had working routines, I chose to build on that foundation, gradually transforming the script into a modular and more reusable package. Instead of switching to C++ or Python-only solutions, I continued using Fortran with \texttt{f2py}.

        One of the primary motivations for writing my own code was flexibility. When I tried to implement a particle spray method in \textit{Galpy}, I found that its performance suffered when using custom potentials not built from its internal C++ combinations. For instance, I wanted to use the \texttt{AllenSantillian} halo model, which was not natively supported. I followed the documentation and wrote a class for it. However, custom potentials bypass \texttt{Galpy}'s C backend, which made the computation very slow and the actions were not computable. This made it clear that developing my own tool would give me more control for non-standard models.

        However, this came with challenges. Once, I wanted to implement potentials from exponential density profiles. These do not admit analytical solutions for the potential. I tried to employ the elliptical coordinate system, so that the potential is just a function of ``distance'' from the center along the equipotential surfaces. While I properly implemented simple potential models in elliptical coordaintes, and naively did not realize that this changes the equations of motion completely given the geometry of the problem. I found my orbits always diverged (barring some special cases). Once I discovered that I must handle the metric tensor and the Christoffel symbols to properly integrate an orbit in elliptical coordaintes, I decided to not invest any more time into this problem and focus on scientific analysis and launching other simulations. This made me realize why many codes, like \texttt{Agama}, and many other papers use a basis function expansion for these potentials. This situation for me, highlights a great example. While I invested much time, and did even obtain the result I wanted, I learned a lot. Below, I would like to highlight some general pros and cons of developing a code from scratch to answer a scientific question. 
        \subsubsection{Advantages and Limitations}
            Writing and maintaining this codebase brought several clear benefits:
            \begin{itemize}
                \item \textit{Understanding.} Developing my own code forced me to really understand all of the modeling techniques. Otherwise, the solutions would not be correct. 
                \item \textit{Flexibility.} I could implement exactly the models and expand as needed
                \item \textit{Transparency.} Results can be verified and reproduced by others using my code.
                \item \textit{Reusability.} Continued development allowed me to identify and fix subtle bugs (e.g., non-symplectic integrators). Some of these bugs are hard to notice since they do not trip error messages and only expose themselves when looking closely or when examining certain edge cases. Working on the same code for prolonged time eliminates these subtle bugs. 
                \item \textit{Collaboration.} A fellow researcher at the Paris observatory is using the code. Additionally, I was aboe to use the code with a master's student and guide them during their semester research project. This would not have been possible without having developed the code in a user friendly way.
                \item \textit{Growth.} This project pushed me to adopt best practices: version control, documentation, modularity, that I will carry into future work. Additionally, developing my own code really makes others easier to understand. For example, I wanted to implement a king model, and I parsed through \texttt{Galpy}'s code base to compare my method. 
            \end{itemize}

            But there are also significant drawbacks:
            \begin{itemize}
                \item \textit{Time cost.} Code development took time away from doing science. Perhaps I could have performed more simulations or analyses instead of improving the code.
                \item \textit{Feature limitations.} I still lack many features common in other packages — such as basis function expansions, action computation, or parallelization strategies like MPI or OpenMP.
                \item \textit{Changing relevance.} Scientific questions and the technology evolve quickly. A tool designed to be general and reusable may become obsolete faster than a single-use script.
            \end{itemize}

            Nonetheless, the code was designed to answer specific scientific questions about Milky Way stellar streams and globular clusters. In the next two chapers, I present how this code was used to improve our understanding of the Milky Way. 


\bibliographystyle{plainnat}
\bibliography{mybib}

\end{document}



