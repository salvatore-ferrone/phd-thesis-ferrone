\documentclass{article}
\usepackage{listings}
\usepackage{natbib}   % For citation management
\usepackage{geometry}
\usepackage{xcolor}
\usepackage{config/psl-cover/aas_macros}
\geometry{a4paper,
  left= 3cm,right = 2cm,  
  top = 3cm,bottom = 3cm,
  headheight=6mm,         
  marginparwidth = 16mm}

\begin{document}

    This project is built on \texttt{f2py} and the paradigm that code compiled in fortran is faster than python, yet python is much more convenient that fortran, particularly given Jupyter notebooks. \texttt{f2py} means \texttt{Fortran} to \texttt{python} \citep{peterson2009f2py}. 

    If the package is correctly installed on the system, then it just need be imported at the head of any python script:
    \small
    \begin{lstlisting}[language=python]
        import tstrippy    
    \end{lstlisting}
    \normalsize

    Then, the user must select and/or load the initial conditions. In the code, I provide the masses, sizes, and kinematics of the globular cluster catalog \citep{2018MNRAS.478.1520B}; the galactic potential parameters for the second model of \citep{2017A&A...598A..66P}; and a galactic reference frame: 
    \small
    \begin{lstlisting}[language=python]
        GCdata      = \
            tstrippy.Parsers.baumgardtMWGCs().data
        MWparams    = \
            tstrippy.Parsers.potential_parameters.pouliasis2017pii()
        MWrefframe  = \
            tstrippy.Parsers.potential_parameters.MWreferenceframe()
    \end{lstlisting}
    \normalsize

    The user must pick the initial conditions. For instance, if the user wants to integrate the globular clusters, they must use the MW reference frame to convert the IRCS coordinates to a Galactocentric reference frame using \texttt{astropy}. Or, the user can generate a plummer sphere if wishing to simulate a cluster: 
    \small
    \begin{lstlisting}[language=python]
        xp,yp,zp,vxp,vyp,vzp=\
        tstrippy.ergodic.isotropicplummer(G,massHost,halfmassradius,NP)
    \end{lstlisting}
    where \texttt{NP} is the number of particles, \texttt{halfmassradius} is the half mass radius of the system, \texttt{massHost} is the total mass of the plummer sphere, and \texttt{G} is of course, the gravitational constant. The values must be in the same unit basis. 

    Then, the integrator must be prepared. All of the arguments can be packaged into a list and then unpacked in the function's arguments. Here is an example of initializing the integrator for a stream in a potential with a galactic bar:
    \small
    \begin{lstlisting}[language=python]
        tstrippy.integrator.setstaticgalaxy(*staticgalaxy)
        tstrippy.integrator.setinitialkinematics(*initialkinematics)
        tstrippy.integrator.setintegrationparameters(*integrationparameters)
        tstrippy.integrator.inithostperturber(*hostperturber)
        tstrippy.integrator.initgalacticbar(*galacticbar)
        tstrippy.integrator.setbackwardorbit()
    \end{lstlisting}
    \normalsize
    \texttt{setstaticgalaxy} tells the code which potential model to use and also passes it's parameters. \texttt{setinitialkinematics} takes the initial positions and velocities of all the particles whose orbits are to be determined. \texttt{integrationparameters} gives the initial time, timestep, and number of steps for the integration. \texttt{inithostperturber} takes the timestamps, positions, velocities, mass, and characteristic radius of the host globular cluster. \texttt{initgalacticbar} is similar to \texttt{setstaticgalaxy}. It expects the name of the bar model, the parameters for the potential, and the parameters for the spin, which are coefficients to a polynomial: $\theta(t) = \theta_0 + \omega t + \dot{\omega}t^2 + \ddot{\omega}t^3 + \dots$, which allows the user to pass a galactic bar with a spin rate that changes in time. \texttt{setbackwardorbit} changes the signs of the velocities to send the bodies backward, and ensures that the internal clock of the integrator decreaes: $t_{i}= t_0 - i\cdot\Delta t$.  

    The user has two options of writing out data during the integration:
    \begin{lstlisting}
        tstrippy.integrator.initwriteparticleorbits(nskip,myoutname,myoutdir)
        tstrippy.integrator.initwritestream(nskip,myoutname,myoutdir)
    \end{lstlisting}
    conceptually, there are two paradigms. We can either same the orbit of each particle into its own file, \textit{or} save snapshots of the simulations at different times. Both functions take the same arguments, \texttt{nskip} controls how many integration timesteps to skip before writing out data. \texttt{myoutdir} is the output directory of where to save the files and \texttt{myoutname} is the base file name, which then gets appended an integer. The resultant filenames would be something like: \texttt{../dir/temp0.bin}, \texttt{../dir/temp1.bin}, \dots \texttt{../dir/tempN.bin}, where $N=N_\mathrm{step}/N_\mathrm{skip}$. \texttt{initwriteparticleorbits} writes the same number of files as the number of particles, and during the integration step it returns to said file and appends it with a particles full phase-space information and timestep. This is intended for saving the trajecotries of the globular clusters. \texttt{initwritestream} on the other hand, is intended for saving the positions and velocities of all the particles at a single timestamp. Note that the data are saved in Fortran binary files. They must be parsed knowing their structure. There is a scipy routine that reads Fortran files, \texttt{scipy.io.FortranFile}. However, I have written a custom script that parses the binary files using \texttt{numpy.frombuffer} to avoid a scipy dependency. 

    After all the parameters the user has two options for integrating. The two options match paradigm for being interested either in orbits of individual bodies, or snapshots of the entire simulation. To save the whole orbit, the user must:
    \small
    \begin{lstlisting}
        xt,yt,zt,vxt,vyt,vzt=\
            tstrippy.integrator.leapfrogintime(Ntimestep,nObj) 
        timestamps=\
            tstrippy.integrator.timestamps.copy()
    \end{lstlisting}
    \normalsize
    \texttt{leapfrogintime} must be used with caution. The resultant data size can be extremely large if the number of steps. For example, if we wanted to integrate the whole globular cluster system at the highest temporal resolution to resolve the internal dynamics of the denesest globular cluster: we would have a data volume of:
    \begin{equation}
        7 \times N_p \times N_\mathrm{step} \times 8~\mathrm{Byte} \approx 450~Gb,
    \end{equation}
    if $N_\mathrm{step}\approx 10^7$. This is much larger than the RAM. This breaks even easier if considering a stream even if at modest timestep size. However, \texttt{leapfrogintime} is useful when wanting to look at orbits of a few particles, or for a short period of time. It is very useful when quickly exploring parameters in a Jupyter notebook. When looking to get high resolution data, writing the data out must happen.  

    To not store any of the intermediate positions in the integrator during the integration we can do this: 
    \small
    \begin{lstlisting}
        tstrippy.integrator.leapfrogtofinalpositions()
        xf  = tstrippy.integrator.xf.copy()
        yf  = tstrippy.integrator.yf.copy()
        zf  = tstrippy.integrator.zf.copy()
        vxf = tstrippy.integrator.vxf.copy()
        vyf = tstrippy.integrator.vyf.copy()
        vzf = tstrippy.integrator.vzf.copy()
        finaltime=tstrippy.integrator.currenttime.copy()
    \end{lstlisting}
    \normalsize
    \texttt{leapfrogtofinalpositions()} just goes to the final positions. The resultant positions must be copied otherwise they will be deleted after deallocating the integrator, which must be done to ensure that the kernel doesn't crash:
    \small
    \begin{lstlisting}
        tstrippy.integrator.deallocate()
    \end{lstlisting}
    \normalsize


\bibliographystyle{plainnat}
\bibliography{mybib}

\end{document}



